{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-15T08:45:16.680244Z",
     "iopub.status.busy": "2025-11-15T08:45:16.680088Z",
     "iopub.status.idle": "2025-11-15T08:46:59.720394Z",
     "shell.execute_reply": "2025-11-15T08:46:59.719475Z",
     "shell.execute_reply.started": "2025-11-15T08:45:16.680228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch torchvision torchaudio transformers peft accelerate bitsandbytes datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T09:59:45.234390Z",
     "iopub.status.busy": "2025-11-15T09:59:45.233615Z",
     "iopub.status.idle": "2025-11-15T09:59:45.848930Z",
     "shell.execute_reply": "2025-11-15T09:59:45.848094Z",
     "shell.execute_reply.started": "2025-11-15T09:59:45.234355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recsys1_user01/recipe_recommender/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "import gc\n",
    "\n",
    "# Очистка памяти\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T09:59:45.850887Z",
     "iopub.status.busy": "2025-11-15T09:59:45.850265Z",
     "iopub.status.idle": "2025-11-15T10:00:08.707964Z",
     "shell.execute_reply": "2025-11-15T10:00:08.707115Z",
     "shell.execute_reply.started": "2025-11-15T09:59:45.850868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:26<00:00,  3.36s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 64,225,280 || all params: 14,832,532,480 || trainable%: 0.4330\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "# Загрузка модели в bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Настраиваем модель для PEFT (Parameter-Efficient Fine-Tuning) с использованием LoRA.\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T10:00:08.709988Z",
     "iopub.status.busy": "2025-11-15T10:00:08.709194Z",
     "iopub.status.idle": "2025-11-15T10:00:09.061362Z",
     "shell.execute_reply": "2025-11-15T10:00:09.060650Z",
     "shell.execute_reply.started": "2025-11-15T10:00:08.709968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 123308/123308 [00:00<00:00, 202211.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет успешно загружен. Количество записей: 80151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80151/80151 [00:23<00:00, 3432.73 examples/s]\n",
      "Filter: 100%|██████████| 80151/80151 [00:00<00:00, 105842.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные отформатированы с использованием chat-шаблона.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"/home/recsys1_user01/recipe_recommender/data-ready_lmm_train.csv\", split=\"train\")\n",
    "dataset = dataset.filter(lambda example: example[\"split\"] == \"train\")\n",
    "print(f\"Датасет успешно загружен. Количество записей: {len(dataset)}\")\n",
    "\n",
    "# Определяем неизменяемую часть промптов\n",
    "SYSTEM_PROMPT = \"\"\"Ты — ИИ-ассистент для системы продуктовых рекомендаций.\n",
    "Твоя задача — анализировать корзину и генерировать идеи для поиска недостающих товаров в виде списка поисковых запросов.\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Контекст:\n",
    "    Текущая корзина: {current_basket}\n",
    "    Прошлые 5 покупок пользователя:\n",
    "    Прошлые 5 наиболее похожих покупок пользователя:\n",
    "### ЗАДАЧА\n",
    "На основе КОНТЕКСТА, сгенерируй не более 20 поисковых запросов, которые помогут пользователю добавить недостающие товары в корзину.\n",
    "Важные правила:\n",
    "1. Запросы должны быть краткими, отражать общие категории или идеи, а не конкретными товарами с брендом или весом.\n",
    "2. Не повторяй товары, которые уже есть в корзине.\n",
    "3. Поисковые запросы должны быть реалистичными для продуктового магазина с 10-15 тысячами наименований\n",
    "4. Вывод должен быть в формате [<запрос 1>, <запрос 2>, ...]\n",
    "\n",
    "### ПРИМЕР (для демонстрации логики, а не для копирования)\n",
    "- Пример входной корзины: [Мука пшеничная; Яйца куриные; Сахар-песок]\n",
    "- Пример правильного вывода: [\"разрыхлитель\", \"ванильный экстракт\", \"сливочное масло\", \"шоколад\", \"кондитерские украшения\"]\"\"\"\n",
    "\n",
    "def format_data_as_messages(example):\n",
    "    try:\n",
    "        input_basket_list = ast.literal_eval(example[\"support_items\"])\n",
    "        target_basket_list = ast.literal_eval(example[\"holdout_items\"])\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {\"text\": None}\n",
    "\n",
    "    # 1. Собираем контент для каждого сообщения\n",
    "    current_basket_str = \"; \".join(input_basket_list)\n",
    "    user_content = USER_PROMPT_TEMPLATE.format(current_basket=current_basket_str)\n",
    "    assistant_content = str(target_basket_list)\n",
    "\n",
    "    # 2. Создаем структуру messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "    ]\n",
    "    # 3. Используем токенизатор для преобразования messages в одну строку для обучения\n",
    "    # add_generation_prompt=False, так как мы предоставляем и ответ ассистента\n",
    "    return { \"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False) }\n",
    "\n",
    "# Применяем новую функцию форматирования\n",
    "dataset = dataset.map(format_data_as_messages)\n",
    "dataset = dataset.filter(lambda example: example[\"text\"] is not None)\n",
    "print(\"Данные отформатированы с использованием chat-шаблона.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T10:00:09.062527Z",
     "iopub.status.busy": "2025-11-15T10:00:09.062234Z",
     "iopub.status.idle": "2025-11-15T10:00:09.068559Z",
     "shell.execute_reply": "2025-11-15T10:00:09.067816Z",
     "shell.execute_reply.started": "2025-11-15T10:00:09.062503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'user_id': 1,\n",
       " 'split': 'train',\n",
       " 'support_items': \"['Чеснок', 'Яйцо куриное', 'Грейпфрут', 'Лук зеленый']\",\n",
       " 'holdout_items': \"['Салат', 'Сыр твердый', 'Майонез']\",\n",
       " 'text': '<|im_start|>system\\nТы — ИИ-ассистент для системы продуктовых рекомендаций.\\nТвоя задача — анализировать корзину и генерировать идеи для поиска недостающих товаров в виде списка поисковых запросов.<|im_end|>\\n<|im_start|>user\\nКонтекст:\\n    Текущая корзина: Чеснок; Яйцо куриное; Грейпфрут; Лук зеленый\\n    Прошлые 5 покупок пользователя:\\n    Прошлые 5 наиболее похожих покупок пользователя:\\n### ЗАДАЧА\\nНа основе КОНТЕКСТА, сгенерируй не более 20 поисковых запросов, которые помогут пользователю добавить недостающие товары в корзину.\\nВажные правила:\\n1. Запросы должны быть краткими, отражать общие категории или идеи, а не конкретными товарами с брендом или весом.\\n2. Не повторяй товары, которые уже есть в корзине.\\n3. Поисковые запросы должны быть реалистичными для продуктового магазина с 10-15 тысячами наименований\\n4. Вывод должен быть в формате [<запрос 1>, <запрос 2>, ...]\\n\\n### ПРИМЕР (для демонстрации логики, а не для копирования)\\n- Пример входной корзины: [Мука пшеничная; Яйца куриные; Сахар-песок]\\n- Пример правильного вывода: [\"разрыхлитель\", \"ванильный экстракт\", \"сливочное масло\", \"шоколад\", \"кондитерские украшения\"]<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n[\\'Салат\\', \\'Сыр твердый\\', \\'Майонез\\']<|im_end|>\\n'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T10:00:09.083721Z",
     "iopub.status.busy": "2025-11-15T10:00:09.083414Z",
     "iopub.status.idle": "2025-11-15T10:00:09.103615Z",
     "shell.execute_reply": "2025-11-15T10:00:09.102812Z",
     "shell.execute_reply.started": "2025-11-15T10:00:09.083699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый пример из датасета:\n",
      "<|im_start|>system\n",
      "Ты — ИИ-ассистент для системы продуктовых рекомендаций.\n",
      "Твоя задача — анализировать корзину и генерировать идеи для поиска недостающих товаров в виде списка поисковых запросов.<|im_end|>\n",
      "<|im_start|>user\n",
      "Контекст:\n",
      "    Текущая корзина: Чеснок; Яйцо куриное; Грейпфрут; Лук зеленый\n",
      "    Прошлые 5 покупок пользователя:\n",
      "    Прошлые 5 наиболее похожих покупок пользователя:\n",
      "### ЗАДАЧА\n",
      "На основе КОНТЕКСТА, сгенерируй не более 20 поисковых запросов, которые помогут пользователю добавить недостающие товары в корзину.\n",
      "Важные правила:\n",
      "1. Запросы должны быть краткими, отражать общие категории или идеи, а не конкретными товарами с брендом или весом.\n",
      "2. Не повторяй товары, которые уже есть в корзине.\n",
      "3. Поисковые запросы должны быть реалистичными для продуктового магазина с 10-15 тысячами наименований\n",
      "4. Вывод должен быть в формате [<запрос 1>, <запрос 2>, ...]\n",
      "\n",
      "### ПРИМЕР (для демонстрации логики, а не для копирования)\n",
      "- Пример входной корзины: [Мука пшеничная; Яйца куриные; Сахар-песок]\n",
      "- Пример правильного вывода: [\"разрыхлитель\", \"ванильный экстракт\", \"сливочное масло\", \"шоколад\", \"кондитерские украшения\"]<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "['Салат', 'Сыр твердый', 'Майонез']<|im_end|>\n",
      "\n",
      "\n",
      "Длина текста: 1228\n",
      "Количество токенов: 450\n"
     ]
    }
   ],
   "source": [
    "print(\"Первый пример из датасета:\")\n",
    "print(dataset[0][\"text\"])\n",
    "print(f\"\\nДлина текста: {len(dataset[0]['text'])}\")\n",
    "\n",
    "# Проверка токенизации\n",
    "sample = dataset[0][\"text\"]\n",
    "tokens = tokenizer(sample, truncation=True, max_length=2048)\n",
    "print(f\"Количество токенов: {len(tokens['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T10:03:51.803506Z",
     "iopub.status.busy": "2025-11-15T10:03:51.803172Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recsys1_user01/recipe_recommender/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset: 100%|██████████| 80151/80151 [00:09<00:00, 8034.15 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 80151/80151 [01:28<00:00, 904.65 examples/s] \n",
      "Truncating train dataset: 100%|██████████| 80151/80151 [00:00<00:00, 155523.97 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='2505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  26/2505 04:16 < 7:21:14, 0.09 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.788600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.675400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m      3\u001b[39m training_args = SFTConfig(\n\u001b[32m      4\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     num_train_epochs=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     packing=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m trainer = SFTTrainer(\n\u001b[32m     27\u001b[39m     model=model,\n\u001b[32m     28\u001b[39m     train_dataset=dataset,\n\u001b[32m     29\u001b[39m     peft_config=peft_config,\n\u001b[32m     30\u001b[39m     args=training_args,\n\u001b[32m     31\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1190\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1189\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recipe_recommender/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    # SFT-specific:\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=2048,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T20:31:02.978455Z",
     "iopub.status.idle": "2025-11-14T20:31:02.978790Z",
     "shell.execute_reply": "2025-11-14T20:31:02.978643Z",
     "shell.execute_reply.started": "2025-11-14T20:31:02.978628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"./qwen_finetuned-bf16\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Модель сохранена в {output_dir}\")\n",
    "\n",
    "# Можно также сохранить полную модель\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T20:31:02.979928Z",
     "iopub.status.idle": "2025-11-14T20:31:02.980293Z",
     "shell.execute_reply": "2025-11-14T20:31:02.980116Z",
     "shell.execute_reply.started": "2025-11-14T20:31:02.980102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Загрузка для инференса\n",
    "model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer_for_inference = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Тестовый пример\n",
    "test_input = ['булочки для бургеров', 'кетчуп', 'горчица', 'лук репчатый', 'помидоры']\n",
    "current_basket_str = \"; \".join(test_input)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(current_basket=current_basket_str)},\n",
    "]\n",
    "\n",
    "# Форматирование промпта\n",
    "prompt = tokenizer_for_inference.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Генерация\n",
    "inputs = tokenizer_for_inference(prompt, return_tensors=\"pt\").to(model_for_inference.device)\n",
    "outputs = model_for_inference.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer_for_inference.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer_for_inference.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(\"Сгенерированный ответ:\", response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8739995,
     "sourceId": 13736201,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
