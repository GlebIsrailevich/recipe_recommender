{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13736201,"sourceType":"datasetVersion","datasetId":8739995}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch torchvision torchaudio trl peft accelerate bitsandbytes unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:45:16.680088Z","iopub.execute_input":"2025-11-15T08:45:16.680244Z","iopub.status.idle":"2025-11-15T08:46:59.720394Z","shell.execute_reply.started":"2025-11-15T08:45:16.680228Z","shell.execute_reply":"2025-11-15T08:46:59.719475Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.9.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gc\nmodel = \"\"\ntokenizer = \"\"\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T09:59:45.233615Z","iopub.execute_input":"2025-11-15T09:59:45.234390Z","iopub.status.idle":"2025-11-15T09:59:45.848930Z","shell.execute_reply.started":"2025-11-15T09:59:45.234355Z","shell.execute_reply":"2025-11-15T09:59:45.848094Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\n\n# Загружаем модель Qwen2-4B.\n# ВАЖНО: load_in_4bit = False, dtype = torch.float16\n# Это требует значительно больше видеопамяти.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"RefalMachine/RuadaptQwen3-4B-Instruct\",\n    max_seq_length = 2048,\n    dtype = torch.float16,   # Указываем тип данных float16\n    load_in_4bit = False,\n    device_map=\"balanced\"\n)\n\n# Настраиваем модель для PEFT (Parameter-Efficient Fine-Tuning) с использованием LoRA.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T09:59:45.850265Z","iopub.execute_input":"2025-11-15T09:59:45.850887Z","iopub.status.idle":"2025-11-15T10:00:08.707964Z","shell.execute_reply.started":"2025-11-15T09:59:45.850868Z","shell.execute_reply":"2025-11-15T10:00:08.707115Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.11.2: Fast Qwen3 patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a3aadbb04f4c0fa6fd7fe386c9fe74"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"import ast\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/input/sirius-formated/llm_ready_recipe_dataset.csv\", split=\"train\")\nprint(f\"Датасет успешно загружен. Количество записей: {len(dataset)}\")\n\n# Определяем неизменяемую часть промптов\nSYSTEM_PROMPT = \"\"\"Ты — ИИ-ассистент для системы продуктовых рекомендаций.\nТвоя задача — анализировать корзину и генерировать идеи для поиска недостающих товаров в виде списка поисковых запросов.\"\"\"\n\nUSER_PROMPT_TEMPLATE = \"\"\"Контекст:\n    Текущая корзина: {current_basket}\n    Прошлые 5 покупок пользователя:\n    Прошлые 5 наиболее похожих покупок пользователя:\n### ЗАДАЧА\nНа основе КОНТЕКСТА, сгенерируй не более 20 поисковых запросов, которые помогут пользователю добавить недостающие товары в корзину.\nВажные правила:\n1. Запросы должны быть краткими, отражать общие категории или идеи, а не конкретными товарами с брендом или весом.\n2. Не повторяй товары, которые уже есть в корзине.\n3. Поисковые запросы должны быть реалистичными для продуктового магазина с 10-15 тысячами наименований\n4. Вывод должен быть в формате [<запрос 1>, <запрос 2>, ...]\n\n### ПРИМЕР (для демонстрации логики, а не для копирования)\n- Пример входной корзины: [Мука пшеничная; Яйца куриные; Сахар-песок]\n- Пример правильного вывода: [\"разрыхлитель\", \"ванильный экстракт\", \"сливочное масло\", \"шоколад\", \"кондитерские украшения\"]\"\"\"\n\ndef format_data_as_messages(example):\n    try:\n        input_basket_list = ast.literal_eval(example[\"input_basket\"])\n        target_basket_list = ast.literal_eval(example[\"target_basket\"])\n    except (ValueError, SyntaxError):\n        return {\"text\": None}\n\n    # 1. Собираем контент для каждого сообщения\n    current_basket_str = \"; \".join(input_basket_list)\n    user_content = USER_PROMPT_TEMPLATE.format(current_basket=current_basket_str)\n    assistant_content = str(target_basket_list)\n\n    # 2. Создаем структуру messages\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": assistant_content},\n    ]\n    # 3. Используем токенизатор для преобразования messages в одну строку для обучения\n    # add_generation_prompt=False, так как мы предоставляем и ответ ассистента\n    return { \"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False) }\n\n# Применяем новую функцию форматирования\ndataset = dataset.map(format_data_as_messages)\ndataset = dataset.filter(lambda example: example[\"text\"] is not None)\nprint(\"Данные отформатированы с использованием chat-шаблона.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:00:08.709194Z","iopub.execute_input":"2025-11-15T10:00:08.709988Z","iopub.status.idle":"2025-11-15T10:00:09.061362Z","shell.execute_reply.started":"2025-11-15T10:00:08.709968Z","shell.execute_reply":"2025-11-15T10:00:09.060650Z"}},"outputs":[{"name":"stdout","text":"Датасет успешно загружен. Количество записей: 123308\nДанные отформатированы с использованием chat-шаблона.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:00:09.062234Z","iopub.execute_input":"2025-11-15T10:00:09.062527Z","iopub.status.idle":"2025-11-15T10:00:09.068559Z","shell.execute_reply.started":"2025-11-15T10:00:09.062503Z","shell.execute_reply":"2025-11-15T10:00:09.067816Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'recipe_url': 'https://www.povarenok.ru/recipes/show/1306/',\n 'input_basket': \"['Яйцо куриное', 'Грейпфрут', 'Салат', 'Майонез']\",\n 'target_basket': \"['Сыр твердый', 'Чеснок', 'Лук зеленый']\",\n 'text': '<|im_start|>system\\nТы — ИИ-ассистент для системы продуктовых рекомендаций.\\nТвоя задача — анализировать корзину и генерировать идеи для поиска недостающих товаров в виде списка поисковых запросов.<|im_end|>\\n<|im_start|>user\\nКонтекст:\\n    Текущая корзина: Яйцо куриное; Грейпфрут; Салат; Майонез\\n    Прошлые 5 покупок пользователя:\\n    Прошлые 5 наиболее похожих покупок пользователя:\\n### ЗАДАЧА\\nНа основе КОНТЕКСТА, сгенерируй не более 20 поисковых запросов, которые помогут пользователю добавить недостающие товары в корзину.\\nВажные правила:\\n1. Запросы должны быть краткими, отражать общие категории или идеи, а не конкретными товарами с брендом или весом.\\n2. Не повторяй товары, которые уже есть в корзине.\\n3. Поисковые запросы должны быть реалистичными для продуктового магазина с 10-15 тысячами наименований\\n4. Вывод должен быть в формате [<запрос 1>, <запрос 2>, ...]\\n\\n### ПРИМЕР (для демонстрации логики, а не для копирования)\\n- Пример входной корзины: [Мука пшеничная; Яйца куриные; Сахар-песок]\\n- Пример правильного вывода: [\"разрыхлитель\", \"ванильный экстракт\", \"сливочное масло\", \"шоколад\", \"кондитерские украшения\"]<|im_end|>\\n<|im_start|>assistant\\n[\\'Сыр твердый\\', \\'Чеснок\\', \\'Лук зеленый\\']<|im_end|>\\n'}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# с данными что-то не то, надо чинить. Ну и вообще как будто стоит взять ноут свой рабочий да адаптировать, а не хуйней заниматься","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:00:09.070043Z","iopub.execute_input":"2025-11-15T10:00:09.070258Z","iopub.status.idle":"2025-11-15T10:00:09.082588Z","shell.execute_reply.started":"2025-11-15T10:00:09.070242Z","shell.execute_reply":"2025-11-15T10:00:09.081775Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(\"Первый пример из датасета:\")\nprint(dataset[0][\"text\"])\nprint(f\"\\nДлина текста: {len(dataset[0]['text'])}\")\n\n# Проверка токенизации\nsample = dataset[0][\"text\"]\ntokens = tokenizer(sample, truncation=True, max_length=2048)\nprint(f\"Количество токенов: {len(tokens['input_ids'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:00:09.083414Z","iopub.execute_input":"2025-11-15T10:00:09.083721Z","iopub.status.idle":"2025-11-15T10:00:09.103615Z","shell.execute_reply.started":"2025-11-15T10:00:09.083699Z","shell.execute_reply":"2025-11-15T10:00:09.102812Z"}},"outputs":[{"name":"stdout","text":"Первый пример из датасета:\n<|im_start|>system\nТы — ИИ-ассистент для системы продуктовых рекомендаций.\nТвоя задача — анализировать корзину и генерировать идеи для поиска недостающих товаров в виде списка поисковых запросов.<|im_end|>\n<|im_start|>user\nКонтекст:\n    Текущая корзина: Яйцо куриное; Грейпфрут; Салат; Майонез\n    Прошлые 5 покупок пользователя:\n    Прошлые 5 наиболее похожих покупок пользователя:\n### ЗАДАЧА\nНа основе КОНТЕКСТА, сгенерируй не более 20 поисковых запросов, которые помогут пользователю добавить недостающие товары в корзину.\nВажные правила:\n1. Запросы должны быть краткими, отражать общие категории или идеи, а не конкретными товарами с брендом или весом.\n2. Не повторяй товары, которые уже есть в корзине.\n3. Поисковые запросы должны быть реалистичными для продуктового магазина с 10-15 тысячами наименований\n4. Вывод должен быть в формате [<запрос 1>, <запрос 2>, ...]\n\n### ПРИМЕР (для демонстрации логики, а не для копирования)\n- Пример входной корзины: [Мука пшеничная; Яйца куриные; Сахар-песок]\n- Пример правильного вывода: [\"разрыхлитель\", \"ванильный экстракт\", \"сливочное масло\", \"шоколад\", \"кондитерские украшения\"]<|im_end|>\n<|im_start|>assistant\n['Сыр твердый', 'Чеснок', 'Лук зеленый']<|im_end|>\n\n\nДлина текста: 1209\nКоличество токенов: 324\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,   # dataset с полем \"text\" (строки)\n    eval_dataset = None,\n    args = SFTConfig(\n        dataset_text_field = \"text\",           # <- важное отличие\n        per_device_train_batch_size = 8,\n        gradient_accumulation_steps = 16,\n        warmup_steps = 5,\n        num_train_epochs=1,                    # или max_steps/num_train_epochs по вашей задаче\n        learning_rate = 2e-4,\n        logging_steps = 10,\n        optim = \"adamw_8bit\",                  # советуют в ноуте\n        weight_decay = 0.001,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\",\n    ),\n)\n\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:03:51.803172Z","iopub.execute_input":"2025-11-15T10:03:51.803506Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n   \\\\   /|    Num examples = 123,308 | Num Epochs = 1 | Total steps = 964\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 16\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 16 x 1) = 128\n \"-____-\"     Trainable parameters = 33,030,144 of 4,040,967,680 (0.82% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  4/964 02:08 < 17:10:08, 0.02 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"model = model.merge_and_unload()\nprint(\"Адаптеры успешно слиты.\")\n\n# Сохраняем итоговую модель для инференса.\noutput_model_path = \"qwen2-4b-recipes-finetuned-float16\"\nmodel.save_pretrained(output_model_path)\ntokenizer.save_pretrained(output_model_path)\n\nprint(f\"Полноценная модель в float16 сохранена в папку: {output_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:31:02.978455Z","iopub.status.idle":"2025-11-14T20:31:02.978790Z","shell.execute_reply.started":"2025-11-14T20:31:02.978628Z","shell.execute_reply":"2025-11-14T20:31:02.978643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Загружаем нашу сохраненную, полноценную модель\noutput_model_path = \"qwen2-4b-recipes-finetuned-float16\" # Убедитесь, что путь верный\nmodel_for_inference = AutoModelForCausalLM.from_pretrained(output_model_path, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer_for_inference = AutoTokenizer.from_pretrained(output_model_path)\n\n\n# --- Формируем тестовый промпт в формате messages ---\ntest_example_input_basket = ['булочки для бургеров', 'кетчуп', 'горчица', 'лук репчатый', 'помидоры']\ncurrent_basket_str = \"; \".join(test_example_input_basket)\nground_truth_for_example = \"['говяжий фарш', 'сыр чеддер', 'листья салата', 'маринованные огурцы', 'картофель фри']\"\n\n# Создаем контент для юзер-промпта\nuser_content_for_inference = USER_PROMPT_TEMPLATE.format(current_basket=current_basket_str)\n\n# Собираем сообщения для инференса (без ответа ассистента)\nmessages_for_inference = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": user_content_for_inference},\n]\n\n# Используем токенизатор для преобразования.\n# add_generation_prompt=True, так как мы хотим, чтобы модель начала генерировать свой ответ.\ninputs = tokenizer_for_inference.apply_chat_template(\n    messages_for_inference,\n    tokenize = True,\n    add_generation_prompt = True,\n    return_tensors = \"pt\"\n).to(model_for_inference.device)\n\n\n# --- Генерируем ответ ---\noutputs = model_for_inference.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n# Декодируем только сгенерированную часть, исключая входной промпт\ndecoded_output = tokenizer_for_inference.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n\n\n# --- Печатаем результат ---\nprint(\"--- Входная корзина ---\")\nprint(test_example_input_basket)\nprint(\"\\n--- Пример правильного ответа (для сравнения) ---\")\nprint(ground_truth_for_example)\nprint(\"\\n--- Сгенерированный ответ моделью ---\")\nprint(decoded_output.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:31:02.979928Z","iopub.status.idle":"2025-11-14T20:31:02.980293Z","shell.execute_reply.started":"2025-11-14T20:31:02.980102Z","shell.execute_reply":"2025-11-14T20:31:02.980116Z"}},"outputs":[],"execution_count":null}]}