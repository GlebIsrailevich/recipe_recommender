{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASE Model Validation Pipeline\n",
    "This notebook validates the EASE model metrics on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sps\n",
    "from tqdm import tqdm\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load the preprocessed data from ease_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grouped data\n",
    "full_grouped_data = pd.read_csv('../recsys_tests/full_data.csv', index_col=0)\n",
    "\n",
    "# Convert string representations back to lists\n",
    "import ast\n",
    "full_grouped_data['train_interactions'] = full_grouped_data['train_interactions'].apply(ast.literal_eval)\n",
    "full_grouped_data['val_interactions'] = full_grouped_data['val_interactions'].apply(ast.literal_eval)\n",
    "full_grouped_data['test_interactions'] = full_grouped_data['test_interactions'].apply(ast.literal_eval)\n",
    "\n",
    "print(f\"Data loaded: {len(full_grouped_data)} users\")\n",
    "full_grouped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(recommendations, ground_truth, k=100):\n",
    "    \"\"\"\n",
    "    Calculate HitRate@k\n",
    "\n",
    "    Args:\n",
    "        recommendations: dict {user_id: list of recommended item_ids}\n",
    "        ground_truth: dict {user_id: set of relevant item_ids}\n",
    "        k: cutoff level\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    total_users = len(recommendations)\n",
    "\n",
    "    for user_id, recs in recommendations.items():\n",
    "        user_recs = recs[:k]\n",
    "        user_truth = ground_truth.get(user_id, set())\n",
    "        if any(item in user_truth for item in user_recs):\n",
    "            hits += 1\n",
    "\n",
    "    return hits / total_users if total_users > 0 else 0.0\n",
    "\n",
    "\n",
    "def precision(recommendations, ground_truth, k=100):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "\n",
    "    for user_id, recs in recommendations.items():\n",
    "        user_recs = recs[:k]\n",
    "        user_truth = ground_truth.get(user_id, set())\n",
    "        relevant_count = sum(1 for item in user_recs if item in user_truth)\n",
    "        user_precision = relevant_count / k\n",
    "        precisions.append(user_precision)\n",
    "\n",
    "    return np.mean(precisions) if precisions else 0.0\n",
    "\n",
    "\n",
    "def recall(recommendations, ground_truth, k=100):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "\n",
    "    for user_id, recs in recommendations.items():\n",
    "        user_recs = recs[:k]\n",
    "        user_truth = ground_truth.get(user_id, set())\n",
    "\n",
    "        if not user_truth:  # If no ground truth items, recall is 0\n",
    "            recalls.append(0.0)\n",
    "            continue\n",
    "\n",
    "        relevant_count = sum(1 for item in user_recs if item in user_truth)\n",
    "        user_recall = relevant_count / len(user_truth)\n",
    "        recalls.append(user_recall)\n",
    "\n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "\n",
    "def mrr(recommendations, ground_truth, k=100):\n",
    "    \"\"\"\n",
    "    Calculate MRR@k\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for user_id, recs in recommendations.items():\n",
    "        user_recs = recs[:k]\n",
    "        user_truth = ground_truth.get(user_id, set())\n",
    "\n",
    "        user_rr = 0.0\n",
    "        for rank, item in enumerate(user_recs, 1):\n",
    "            if item in user_truth:\n",
    "                user_rr = 1.0 / rank\n",
    "                break\n",
    "\n",
    "        reciprocal_ranks.append(user_rr)\n",
    "\n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "def ndcg(recommendations, ground_truth, k=100, binary_relevance=True):\n",
    "    \"\"\"\n",
    "    Calculate NDCG@k\n",
    "\n",
    "    Args:\n",
    "        binary_relevance: if True, uses binary relevance (0/1),\n",
    "                         if False, expects relevance scores in ground_truth\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for user_id, recs in recommendations.items():\n",
    "        user_recs = recs[:k]\n",
    "        user_truth = ground_truth.get(user_id, {})\n",
    "\n",
    "        # Calculate DCG\n",
    "        dcg = 0.0\n",
    "        for rank, item in enumerate(user_recs, 1):\n",
    "            if binary_relevance:\n",
    "                rel = 1.0 if item in user_truth else 0.0\n",
    "            else:\n",
    "                rel = user_truth.get(item, 0.0)\n",
    "\n",
    "            dcg += rel / (log2(rank + 1) if rank == 1 else 1)\n",
    "\n",
    "        # Calculate IDCG\n",
    "        if binary_relevance:\n",
    "            # For binary relevance, ideal is all 1's sorted first\n",
    "            num_relevant = len(user_truth)\n",
    "            ideal_gains = [1.0] * min(k, num_relevant)\n",
    "        else:\n",
    "            # For graded relevance, take top-k relevance scores\n",
    "            ideal_gains = sorted(user_truth.values(), reverse=True)[:k]\n",
    "\n",
    "        idcg = 0.0\n",
    "        for rank, rel in enumerate(ideal_gains, 1):\n",
    "            idcg += rel / (log2(rank + 1) if rank == 1 else 1)\n",
    "\n",
    "        user_ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcg_scores.append(user_ndcg)\n",
    "\n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    df: pd.DataFrame, preds_col: str, gt_col: str, top_k: int = 20\n",
    ") -> dict:\n",
    "    recommendations = pd.Series(df[preds_col].values, index=df[\"user_id\"]).to_dict()\n",
    "\n",
    "    ground_truth = pd.Series(\n",
    "        df[gt_col].apply(set).values, index=df[\"user_id\"]\n",
    "    ).to_dict()\n",
    "\n",
    "    hr = hit_rate(recommendations, ground_truth, k=top_k)\n",
    "    p = precision(recommendations, ground_truth, k=top_k)\n",
    "    r = recall(recommendations, ground_truth, k=top_k)\n",
    "    m = mrr(recommendations, ground_truth, k=top_k)\n",
    "    n = ndcg(recommendations, ground_truth, k=top_k)\n",
    "\n",
    "    results = {\n",
    "        f\"hit_rate@{top_k}\": hr,\n",
    "        f\"precision@{top_k}\": p,\n",
    "        f\"recall@{top_k}\": r,\n",
    "        f\"mrr@{top_k}\": m,\n",
    "        f\"ndcg@{top_k}\": n,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct mappings and matrix\n",
    "We need to reconstruct the item2id and id2item mappings, as well as the training matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique items from all interactions\n",
    "all_items = set()\n",
    "for interactions in full_grouped_data['train_interactions']:\n",
    "    all_items.update(interactions)\n",
    "\n",
    "# Create sorted list for consistent mapping\n",
    "unique_items = sorted(list(all_items))\n",
    "item2id = {item: idx for idx, item in enumerate(unique_items)}\n",
    "id2item = {idx: item for item, idx in item2id.items()}\n",
    "\n",
    "# Get user mapping\n",
    "unique_users = full_grouped_data['user_id'].values\n",
    "user2id = {user: idx for idx, user in enumerate(unique_users)}\n",
    "id2user = {idx: user for user, idx in user2id.items()}\n",
    "\n",
    "print(f\"Number of items: {len(item2id)}\")\n",
    "print(f\"Number of users: {len(user2id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training matrix from train interactions\n",
    "rows = []\n",
    "cols = []\n",
    "\n",
    "for idx, row in full_grouped_data.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    user_idx = user2id[user_id]\n",
    "    for item in row['train_interactions']:\n",
    "        if item in item2id:\n",
    "            rows.append(user_idx)\n",
    "            cols.append(item2id[item])\n",
    "\n",
    "values = np.ones(len(rows))\n",
    "matrix_train = sps.coo_matrix(\n",
    "    (values, (rows, cols)),\n",
    "    shape=(len(user2id), len(item2id)),\n",
    "    dtype=np.float64\n",
    ")\n",
    "\n",
    "print(f\"Training matrix shape: {matrix_train.shape}\")\n",
    "print(f\"Number of interactions: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EASE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def fit_ease(X, reg_weight=100):\n",
    "    \"\"\"\n",
    "    Train EASE model using RecBole implementation\n",
    "    \n",
    "    Args:\n",
    "        X: User-item interaction matrix (users x items)\n",
    "        reg_weight: Regularization weight (default: 100)\n",
    "    \n",
    "    Returns:\n",
    "        B: Item-item weight matrix\n",
    "    \"\"\"\n",
    "    # gram matrix\n",
    "    G = X.T @ X\n",
    "\n",
    "    # add reg to diagonal\n",
    "    G += reg_weight * sps.identity(G.shape[0])\n",
    "\n",
    "    # convert to dense because inverse will be dense\n",
    "    G = G.todense()\n",
    "\n",
    "    # invert. this takes most of the time\n",
    "    P = np.linalg.inv(G)\n",
    "    B = P / (-np.diag(P))\n",
    "    # zero out diag\n",
    "    np.fill_diagonal(B, 0.)\n",
    "    \n",
    "    return B\n",
    "\n",
    "w = fit_ease(matrix_train)\n",
    "print(f\"Model weights shape: {w.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(user_interactions, item2id, id2item, model_weights):\n",
    "    \"\"\"\n",
    "    Generate predictions for a user\n",
    "    \n",
    "    Args:\n",
    "        user_interactions: List of item IDs the user has interacted with\n",
    "        item2id: Dictionary mapping items to indices\n",
    "        id2item: Dictionary mapping indices to items\n",
    "        model_weights: Trained EASE weight matrix\n",
    "    \n",
    "    Returns:\n",
    "        top_indices: Top 20 recommended item indices\n",
    "    \"\"\"\n",
    "    encoded_ids = user_interactions\n",
    "    \n",
    "    vector = np.zeros(len(item2id))\n",
    "    vector[encoded_ids] = 1\n",
    "    \n",
    "    preds = vector @ model_weights\n",
    "    preds[encoded_ids] = -np.inf  # Filter out items already seen\n",
    "    \n",
    "    top_indices = np.argsort(-preds)[:20]\n",
    "    \n",
    "    decoded = [id2item[i] for i in top_indices]\n",
    "    \n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "w = np.asarray(w)\n",
    "\n",
    "tqdm.pandas()\n",
    "full_grouped_data['ease_preds'] = full_grouped_data['train_interactions'].progress_apply(\n",
    "    lambda interactions: get_preds(interactions, item2id, id2item, w)\n",
    ")\n",
    "full_grouped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate at top_k=10\n",
    "metrics = evaluate_model(full_grouped_data, 'ease_preds', 'test_interactions', top_k=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EASE Model Performance on Test Set @ k=10\")\n",
    "print(\"=\"*50)\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name:20s}: {value}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display as dictionary\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "The metrics should match:\n",
    "```python\n",
    "{\n",
    " 'hit_rate@10': 0.696154871653995,\n",
    " 'precision@10': 0.092590108833534,\n",
    " 'recall@10': 0.4643900770533879,\n",
    " 'mrr@10': 0.37347038489245216,\n",
    " 'ndcg@10': 0.4643900770533879\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify metrics match expected values\n",
    "expected_metrics = {\n",
    "    'hit_rate@10': 0.696154871653995,\n",
    "    'precision@10': 0.092590108833534,\n",
    "    'recall@10': 0.4643900770533879,\n",
    "    'mrr@10': 0.37347038489245216,\n",
    "    'ndcg@10': 0.4643900770533879\n",
    "}\n",
    "\n",
    "print(\"\\nValidation Check:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<20} {'Expected':<20} {'Actual':<20} {'Match':<10}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tolerance = 1e-10\n",
    "all_match = True\n",
    "\n",
    "for metric_name, expected_value in expected_metrics.items():\n",
    "    actual_value = metrics[metric_name]\n",
    "    match = abs(actual_value - expected_value) < tolerance\n",
    "    all_match = all_match and match\n",
    "    match_str = \"PASS\" if match else \"FAIL\"\n",
    "    print(f\"{metric_name:<20} {expected_value:<20.15f} {actual_value:<20.15f} {match_str:<10}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "if all_match:\n",
    "    print(\"PASS: All metrics match expected values!\")\n",
    "else:\n",
    "    print(\"FAIL: Some metrics do not match. Check data preprocessing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
